from utils.logging import logger
from datetime import datetime
from config import RSS_FEEDS,CHANNEL_ID
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from googletrans import Translator
from setup_database import is_news_processed, mark_news_as_processed
from utils.helpers import strip_html_tags_and_unescape_entities
import email.utils as eut
import hashlib
import feedparser
import torch
import asyncio
import re


translator = Translator()
news_lock = asyncio.Lock()



#Model for sentiment analysis(Ù…ÙˆØ¯ÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±)
model_name = "ProsusAI/finbert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
sentiment_labels = [model.config.id2label[i] for i in range(len(model.config.id2label))]#["positive","negative","neutral"]







async def fetch_news_from_rss():
    """
    -------------------------
    ÙˆØ¸ÙŠÙØ© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± RSS
    -------------------------
    """
    news_list = []
    try:
        for feed_url in RSS_FEEDS:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries:
                uniq_id = hashlib.md5((entry.title + entry.link + entry.published).encode()).hexdigest()
                if is_news_processed(uniq_id):
                    logger.info(f"News already processed: {entry.title}")
                    continue

                image_url = None
                if 'media_content' in entry and entry['media_content']:
                    for media in entry['media_content']:
                        if media.get('type', '').startswith('image/') and media.get('url'):
                            image_url = media['url']
                            break
                elif 'enclosures' in entry and entry['enclosures']:
                    for enclosure in entry['enclosures']:
                        if enclosure.get('type', '').startswith('image/') and enclosure.get('url'):
                            image_url = enclosure['url']
                            break
                
                # Fallback: try to find an <img> tag in the summary
                if not image_url and entry.summary:
                    img_match = re.search(r'<img[^>]+src=["\'](.*?)["\']', entry.summary, re.IGNORECASE)
                    if img_match:
                        image_url = img_match.group(1)


                # Convert published time to 12-hour format
                if entry.get('published_parsed'):
                    dt = datetime(*entry.published_parsed[:6])
                else:
                    dt = datetime.fromtimestamp(eut.mktime_tz(eut.parsedate_tz(entry.get('published', ''))))

                published_12h = dt.strftime("%a, %d %b %Y â€¢ %I:%M %p")

                summary_text = entry.summary or entry.description or entry.title #Ù…ÙˆÙ‚Ø¹ coindesk Ù…Ø§ ÙŠØ¹Ø·ÙŠ summary Ø§Ùˆ description Ù„Ø°Ù„Ùƒ Ù†Ø±Ø³Ù„ title ÙƒØ§ Ø­Ù„ Ø§Ø®ÙŠØ±
                
                news_list.append({
                    "uniq_id": uniq_id,
                    "title": entry.title,
                    "link": entry.link,
                    "published": published_12h or "Time of publishing NOT FOUND",
                    "summary": strip_html_tags_and_unescape_entities(summary_text), 
                    "image_url": image_url,
                })

        return news_list
    except Exception as e:
        logger.error(f"Error in fetch_news_from_rss: {e}")
        return []




async def analyze_news_with_ProsusAI_finbert_ai(news): 
     """
     -------------------------
     ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ProsusAI/finbert
     -------------------------
     """ 
     try: 
         def run_analysis(): 
             inputs = tokenizer( 
                 news["summary"], 
                 return_tensors="pt", 
                 truncation=True, 
                 padding=True, 
                 max_length=512
             ) 
             with torch.no_grad(): 
                 outputs = model(**inputs) 
                 predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) 
                 sentiment_idx = predictions.argmax(dim=-1).item() 
             return { 
                 "sentiment": sentiment_labels[sentiment_idx], 
                 "confidence": predictions[0][sentiment_idx].item() 
             } 



         return await asyncio.to_thread(run_analysis) 
     except Exception as e: 
         logger.error(f"Error in analyze_news_with_ProsusAI_finbert_ai: {e}") 
         return {"sentiment": "Error", "confidence": 0.0} 
 
async def news_job(context): 
    async with news_lock:
        """
        -------------------------
        Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        -------------------------
        """
        chat_id = CHANNEL_ID  # Ø§Ø³ØªØ®Ø¯Ø§Ù… CHANNEL_ID Ø§Ù„Ù…Ø³ØªÙˆØ±Ø¯ Ù…Ù† config.py
        if not chat_id:
            logger.warning("CHANNEL_ID is not set. Cannot send news message.")
            return []

        try: 
            news_list = await fetch_news_from_rss() 
            if not news_list: 
                return [] 



            tasks = [analyze_news_with_ProsusAI_finbert_ai(news) for news in news_list] 
            results = await asyncio.gather(*tasks) 



            for news, analysis in zip(news_list, results): 
                original_title = news.get('title','') # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ØµÙ„ÙŠ

                # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                try:
                    title_ar = (await translator.translate(original_title, dest='ar')).text
                except Exception as e:
                    title_ar = '' #ÙŠØ­Ø°Ù Ø§Ù„Ù†Øµ Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„ Ùˆ ÙŠÙƒÙ…Ù„ Ø·Ø¨ÙŠØ¹ÙŠ
                    logger.error(f"Error in translating title to Arabic: {e}")



                # ØªÙ‡Ø±ÙŠØ¨ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… HTML
                safe_title_en = strip_html_tags_and_unescape_entities(original_title)
                safe_title_ar = strip_html_tags_and_unescape_entities(title_ar) if title_ar else '' #  ÙŠØªØ§ÙƒØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºÙ‹Ø§ØŒ Ø³ÙŠÙƒÙˆÙ† ÙØ§Ø±ØºÙ‹Ø§ Ø£ÙŠØ¶Ù‹Ø§ 

                summary_text = strip_html_tags_and_unescape_entities(news['summary'])
                safe_summary = strip_html_tags_and_unescape_entities(summary_text)
                
                #ÙƒØªØ§Ø¨Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¨Ø± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ
                sentiment_arabic_map = {
                    "positive": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
                    "negative": "Ø³Ù„Ø¨ÙŠ",
                    "neutral": "Ù…Ø­Ø§ÙŠØ¯"
                }
                safe_sentiment_arabic = sentiment_arabic_map.get(analysis['sentiment'], "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")

                # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ø®Øµ Ù„Ù„ØµÙˆØ± (caption Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù€ 1024 Ø­Ø±Ù)
                if len(safe_summary) > 600:  
                    safe_summary = safe_summary[:600] + "..." 

                # emoji_status = "ğŸ”´" if analysis['sentiment'] == "negative" else ("ğŸŸ¢" if analysis['sentiment'] == "positive" else "âšª") Ø·Ø±ÙŠÙ‚Ø© Ø§Ø®Ø±Ù‰ Ù„ÙƒÙ† Ø¨Ø¬Ø±Ø¨ switch 
                switch = {
                    "negative": "ğŸ”´",
                    "positive": "ğŸŸ¢",
                    "neutral": "âšª",
                }
                emoji_status = switch.get(analysis['sentiment'], "âšª")
                
                safe_published = strip_html_tags_and_unescape_entities(news['published'])
                safe_sentiment = strip_html_tags_and_unescape_entities(analysis['sentiment'])
                safe_confidence = f"{analysis['confidence']:.2%}"
                safe_link = news['link']  # Ù„Ø§ Ù†Ø­ØªØ§Ø¬ escape Ù„Ù„Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ HTML tag

                title_section = f"<b>ğŸ‡¸ğŸ‡¦ {safe_title_ar}</b>\n <b>ğŸ‡¬ğŸ‡§ {safe_title_en}</b>\n" if safe_title_ar else f"<b>ğŸ‡¬ğŸ‡§ {safe_title_en}</b>\n"

                # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø¨ØµÙŠØºØ© HTML

                caption = (
                    f"ğŸ— Ø§Ù„Ø¹Ù†ÙˆØ§Ù†(Title) : \n{title_section}\n"
                    f"ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±(Published) : {safe_published}\n"
                    f"ğŸ“° {safe_summary}\n"
                    f"\n ğŸ¤– ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± (News Analysis) :  \n"
                    f"ğŸ” Ø´Ø¹ÙˆØ± Ø§Ù„Ø®Ø¨Ø± (News Sentiment) : \n{safe_sentiment_arabic} ({safe_sentiment}) {emoji_status}\n"
                    f"ğŸ“Š Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø´Ø¹ÙˆØ± Ø§Ù„Ø®Ø¨Ø± (Confidence) : {safe_confidence}\n"
                    f"ğŸ”— <a href=\"{safe_link}\">Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯</a>"
                )


                try: 
                    if news['image_url']:
                        # Ø¥Ø±Ø³Ø§Ù„ ØµÙˆØ±Ø© Ù…Ø¹ caption
                        await context.bot.send_photo(
                            chat_id=chat_id,
                            photo=news['image_url'],
                            caption=caption,
                            parse_mode="HTML"
                        )
                    else:
                        # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ù†ØµÙŠØ© Ø¨Ø¯ÙˆÙ† ØµÙˆØ±Ø©
                        await context.bot.send_message(
                            chat_id=chat_id,
                            text=caption,
                            parse_mode="HTML",
                            disable_web_page_preview=True  # âœ… ÙÙ‚Ø· ÙÙŠ send_message
                            )
                            
                    mark_news_as_processed(news['uniq_id'], news['title'], news['link'])
                except Exception as e:
                    logger.error(f"Error sending message: {e}")



            return news_list
        except Exception as e:
            logger.error(f"Error in news_job: {e}")
            return []

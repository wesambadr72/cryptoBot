from utils.logging import logger
import feedparser
import hashlib
from datetime import datetime
from handlers import CHAT_ID
from config import RSS_FEEDS
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import asyncio
import re

#Model for sentiment analysis(Ù…ÙˆØ¯ÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±)
model_name = "ProsusAI/finbert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
sentiment_labels = [model.config.id2label[i] for i in range(len(model.config.id2label))]#["positive","negative","neutral"]


# list for storing seen news
seen_news = set()

# Function to escape special characters in Markdown( Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© ÙÙŠ Markdown)
def escape_markdown(text: str) -> str:
    # ØªÙ‡Ø±ÙŠØ¨ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© ÙÙŠ MarkdownV2 Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ ØªÙ„Ùƒ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    # Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ ØªÙ‡Ø±ÙŠØ¨Ù‡Ø§: _, *, [, ], (, ), ~, `, >, #, +, -, =, |, {, }, ., !
    # ÙˆÙ„ÙƒÙ† ( Ùˆ ) Ù„Ø§ ÙŠØ¬Ø¨ ØªÙ‡Ø±ÙŠØ¨Ù‡Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Ø§ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø±Ø§Ø¨Ø· [Ù†Øµ](Ø±Ø§Ø¨Ø·)
    # Ù‡Ø°Ø§ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø§Ù„Ø¹Ø§Ø¯ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‡Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ ØªÙ„Ùƒ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    # Ù‡Ø°Ø§ Ø­Ù„ Ù…Ø¨Ø³Ø· ÙˆÙ‚Ø¯ Ù„Ø§ ÙŠØºØ·ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ù„Ù€ MarkdownV2
    return re.sub(r'([_*\[\]()~`>#+\-=|{}.!])', r'\\\1', text)


async def fetch_news_from_rss():
    """
    -------------------------
    ÙˆØ¸ÙŠÙØ© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± RSS
    -------------------------
    """
    news_list = []
    try:
        for feed_url in RSS_FEEDS:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries:
                uniq_id = hashlib.md5((entry.title + entry.link).encode()).hexdigest()
                if uniq_id not in seen_news:
                    news_list.append({
                        "title": entry.title,
                        "link": entry.link,
                        "published": entry.get("published") or "Time of publishing NOT FOUND",
                        "summary": entry.summary,
                    })
                    seen_news.add(uniq_id)
        return news_list
    except Exception as e:
        logger.error(f"Error in fetch_news_from_rss: {e}")
        return []


async def analyze_news_with_ProsusAI_finbert_ai(news): 
     """
     -------------------------
     ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ProsusAI/finbert
     -------------------------
     """ 
     try: 
         def run_analysis(): 
             inputs = tokenizer( 
                 news["summary"], 
                 return_tensors="pt", 
                 truncation=True, 
                 padding=True, 
                 max_length=256 
             ) 
             with torch.no_grad(): 
                 outputs = model(**inputs) 
                 predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) 
                 sentiment_idx = predictions.argmax(dim=-1).item() 
             return { 
                 "sentiment": sentiment_labels[sentiment_idx], 
                 "confidence": predictions[0][sentiment_idx].item() 
             } 

         return await asyncio.to_thread(run_analysis) 
     except Exception as e: 
         logger.error(f"Error in analyze_news_with_ProsusAI_finbert_ai: {e}") 
         return {"sentiment": "Error", "confidence": 0.0} 
 
async def news_job(context): 
    """
    -------------------------
    Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    -------------------------
    """
    if not CHAT_ID: 
        logger.warning("CHAT_ID is not set. Cannot send news message.") 
        return [] 

    try: 
        news_list = await fetch_news_from_rss() 
        if not news_list: 
            return [] 

        tasks = [analyze_news_with_ProsusAI_finbert_ai(news) for news in news_list] 
        results = await asyncio.gather(*tasks) 

        for news, analysis in zip(news_list, results): 
            safe_title = escape_markdown(news['title']) 
            safe_summary = escape_markdown(news['summary']) 
            if len(safe_summary) > 1000: 
                safe_summary = safe_summary[:1000] + "\\.\\.\\." 
            
            message = ( 
                f"ğŸ— Ø§Ù„Ø¹Ù†ÙˆØ§Ù† : *{safe_title}*\n\n" 
                f"ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø± : {news['published']}\n" 
                f"ğŸ“° {safe_summary}\n\n" 
                f"ğŸ” Ø´Ø¹ÙˆØ± Ø§Ù„Ø®Ø¨Ø± : {analysis['sentiment']}\n" 
                f"ğŸ“Š Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø´Ø¹ÙˆØ± Ø§Ù„Ø®Ø¨Ø± : {analysis['confidence']:.2%}\n" 
                f"ğŸ”— [Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯]({news['link']})" 
            ) 

            try: 
                await context.bot.send_message( 
                    chat_id=CHAT_ID, 
                    text=message, 
                    parse_mode="MarkdownV2", 
                    disable_web_page_preview=True 
                ) 
            except Exception as e: 
                logger.error(f"Error sending message: {e}") 
                await context.bot.send_message( 
                    chat_id=CHAT_ID, 
                    text=f"{news['title']}\n{news['link']}" 
                ) 

        return news_list 
    except Exception as e: 
        logger.error(f"Error in news_job: {e}")
        return []